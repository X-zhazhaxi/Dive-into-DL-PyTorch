{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_8yq6tup",
    "id": "5CB587E1495B436F8C823F70CCE5DBC1",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 11.6 Momentum\n",
    "\n",
    "目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降（steepest descent）。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。对于noisy gradient,我们需要谨慎的选取学习率和batch size, 来控制梯度方差和收敛的结果。\n",
    "\n",
    "SGD中 mini_batch 和 full_batch 存在偏差 -> min_batch_gradient也存在 noise -> sgd无法收敛到最优点\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{g}_t = \\partial_{\\mathbf{w}} \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} f(\\mathbf{x}_{i}, \\mathbf{w}_{t-1}) = \\frac{1}{|\\mathcal{B}_t|} \\sum_{i \\in \\mathcal{B}_t} \\mathbf{g}_{i, t-1}.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_ugnt88i",
    "id": "719E334CAB294EEA96D4EAC794CB6C4B",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## An ill-conditioned Problem\n",
    "\n",
    "Condition Number of Hessian Matrix:\n",
    "   \n",
    "$$\n",
    " cond_{H} = \\frac{\\lambda_{max}}{\\lambda_{min}} \n",
    "$$\n",
    " \n",
    "where $\\lambda_{max}, \\lambda_{min}$ is the maximum amd minimum eignvalue of Hessian matrix.\n",
    "\n",
    "condition number 是一个矩阵（或者它所描述的线性系统）的稳定性或者敏感度的度量，如果一个矩阵的 condition number 在1附近，那么它就是well-conditioned的，如果远大于1，那么它就是 ill-conditioned 的，如果一个系统是 ill-conditioned 的，则它的输出结果可信度低。\n",
    "\n",
    "让我们考虑一个输入和输出分别为二维向量$\\boldsymbol{x} = [x_1, x_2]^\\top$和标量的目标函数:\n",
    "\n",
    "\n",
    "$$\n",
    " f(\\boldsymbol{x})=0.1x_1^2+2x_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    " cond_{H} = \\frac{4}{0.2} = 20 \\quad \\rightarrow \\quad \\text{ill-conditioned} \n",
    "$$\n",
    " \n",
    "ill-conditioned -> 导致训练时，函数在$x_1 x_2$方向上的收敛速度偏差很大，难以设定合适的学习率\n",
    "## Maximum Learning Rate\n",
    "+ For $f(x)$, according to convex optimizaiton conclusions, we need step size $\\eta < \\frac{1}{L}$to have the fastest convergence where $ L= max_x \\nabla^2f(x)$ called smooth.\n",
    "+ To guarantee the convergence, we need to have $\\eta < \\frac{2}{L}$ .\n",
    "\n",
    "## Supp: Preconditioning\n",
    "\n",
    "二阶优化的核心思想：在二阶优化中，我们使用Hessian matrix的逆矩阵(或者pseudo inverse)来左乘梯度向量 $i.e. \\Delta_{x} = H^{-1}\\mathbf{g}$，这样的做法称为precondition，相当于将 $H$ 映射为一个单位矩阵，拥有分布均匀的Spectrum，也即我们去优化的等价标函数的Hessian matrix为良好的identity matrix。   \n",
    "将相对扁平的优化下突点（即 长轴>>短轴 的椭圆）变为相对圆的优化下突点（更圆润）-> 减小 condition number -> 可以取更大的学习率\n",
    "\n",
    "\n",
    "与[Section 11.4](https://d2l.ai/chapter_optimization/sgd.html#sec-sgd)一节中不同，这里将$x_1^2$系数从$1$减小到了$0.1$。下面实现基于这个目标函数的梯度下降，并演示使用学习率为$0.4$时自变量的迭代轨迹。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "graffitiCellId": "id_kujf7mb",
    "id": "14F1B2ECD12E4F64B054F7421F312BF7",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, x1 -0.943467, x2 -0.000073\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"/home/xixixi/桌面/Dive-into-DL-PyTorch/code\")\n",
    "import d2lzh_pytorch as d2l\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "eta = 0.4\n",
    "\n",
    "def f_2d(x1, x2):\n",
    "    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "def gd_2d(x1, x2, s1, s2):\n",
    "    return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)\n",
    "\n",
    "d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_vkw2adz",
    "id": "0F0DD4E4F0794763B16067916DDFBFD1",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "可以看到，同一位置上，目标函数在竖直方向（$x_2$轴方向）比在水平方向（$x_1$轴方向）的斜率的绝对值更大。因此，给定学习率，梯度下降迭代自变量时会使自变量在竖直方向比在水平方向移动幅度更大。那么，我们需要一个较小的学习率从而避免自变量在竖直方向上越过目标函数最优解。然而，这会造成自变量在水平方向上朝最优解移动变慢。\n",
    "\n",
    "下面我们试着将学习率调得稍大一点，此时自变量在竖直方向不断越过最优解并逐渐发散。\n",
    "\n",
    "### Solution to ill-condition\n",
    "+ __Preconditioning gradient vector__: applied in Adam, RMSProp, AdaGrad, Adelta, KFC, Natural gradient and other secord-order optimization algorithms.\n",
    "+ __Averaging history gradient__: like momentum, which allows larger learning rates to accelerate convergence; applied in Adam, RMSProp, SGD momentum.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "graffitiCellId": "id_d8n5yky",
    "id": "A293DD7BF95245F998F279897A8A8359",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, x1 -0.387814, x2 -1673.365109\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/A293DD7BF95245F998F279897A8A8359/q5qnxcgkw7.png\">"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eta = 0.6\n",
    "d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "graffitiCellId": "id_rwoy2z0",
    "id": "F4FC92274AE7486A94298E8A655E4365",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Momentum Algorithm\n",
    "\n",
    "动量法的提出是为了解决梯度下降的上述问题。设时间步 $t$ 的自变量为 $\\boldsymbol{x}_t$，学习率为 $\\eta_t$。\n",
    "在时间步 $t=0$，动量法创建速度变量 $\\boldsymbol{m}_0$，并将其元素初始化成 0。在时间步 $t>0$，动量法对每次迭代的步骤做如下修改：\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{m}_t &\\leftarrow \\beta \\boldsymbol{m}_{t-1} + \\eta_t \\boldsymbol{g}_t, \\\\\n",
    "\\boldsymbol{x}_t &\\leftarrow \\boldsymbol{x}_{t-1} - \\boldsymbol{m}_t,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Another version:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{m}_t &\\leftarrow \\beta \\boldsymbol{m}_{t-1} + (1-\\beta) \\boldsymbol{g}_t, \\\\\n",
    "\\boldsymbol{x}_t &\\leftarrow \\boldsymbol{x}_{t-1} - \\alpha_t \\boldsymbol{m}_t,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\frac{\\eta_t}{1-\\beta} \n",
    "$$\n",
    "\n",
    "其中，动量超参数 $\\beta$满足 $0 \\leq \\beta < 1$。当 $\\beta=0$ 时，动量法等价于小批量随机梯度下降。\n",
    "\n",
    "在解释动量法的数学原理前，让我们先从实验中观察梯度下降在使用动量法后的迭代轨迹。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "graffitiCellId": "id_gzuie5r",
    "id": "337EA9AC35D849FA84C00D0811DF3F33",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, x1 -0.062843, x2 0.001202\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/337EA9AC35D849FA84C00D0811DF3F33/q5qnz4bzn0.png\">"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def momentum_2d(x1, x2, v1, v2):\n",
    "    v1 = beta * v1 + eta * 0.2 * x1\n",
    "    v2 = beta * v2 + eta * 4 * x2\n",
    "    return x1 - v1, x2 - v2, v1, v2\n",
    "\n",
    "eta, beta = 0.4, 0.5\n",
    "d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_bn8rcsq",
    "id": "CA38E42BA60541E388FF6CCEC87E18E1",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "可以看到使用较小的学习率 $\\eta=0.4$ 和动量超参数 $\\beta=0.5$ 时，动量法在竖直方向上的移动更加平滑，且在水平方向上更快逼近最优解。下面使用较大的学习率 $\\eta=0.6$，此时自变量也不再发散。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "graffitiCellId": "id_7ckrl4v",
    "id": "B3D7EA65B09743499C1FA28F723B3DF6",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, x1 0.007188, x2 0.002553\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/B3D7EA65B09743499C1FA28F723B3DF6/q5qnzu2jzs.png\">"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eta = 0.6\n",
    "d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_n4u0k3a",
    "id": "C4193FFE1E9441DA8412BC2D10AB3DCA",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Exponential Moving Average\n",
    "\n",
    "为了从数学上理解动量法，让我们先解释一下指数加权移动平均（exponential moving average）。给定超参数 $0 \\leq \\beta < 1$，当前时间步 $t$ 的变量 $y_t$ 是上一时间步 $t-1$ 的变量 $y_{t-1}$ 和当前时间步另一变量 $x_t$ 的线性组合：\n",
    "\n",
    "$$\n",
    "y_t = \\beta y_{t-1} + (1-\\beta) x_t.\n",
    "$$\n",
    "\n",
    "我们可以对 $y_t$ 展开：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_t  &= (1-\\beta) x_t + \\beta y_{t-1}\\\\\n",
    "         &= (1-\\beta)x_t + (1-\\beta) \\cdot \\beta x_{t-1} + \\beta^2y_{t-2}\\\\\n",
    "         &= (1-\\beta)x_t + (1-\\beta) \\cdot \\beta x_{t-1} + (1-\\beta) \\cdot \\beta^2x_{t-2} + \\beta^3y_{t-3}\\\\\n",
    "         &= (1-\\beta) \\sum_{i=0}^{t-1} \\beta^{i}x_{t-i}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(1-\\beta)\\sum_{i=0}^{t-1} \\beta^{i} = \\frac{1-\\beta^{t}}{1-\\beta} (1-\\beta) = (1-\\beta^{t})\n",
    "$$\n",
    "\n",
    "### SuppApproximate Average of $\\frac{1}{1-\\beta}$ Steps\n",
    "\n",
    "令 $n = 1/(1-\\beta)$，那么 $\\left(1-1/n\\right)^n = \\beta^{1/(1-\\beta)}$。因为\n",
    "\n",
    "$$\n",
    " \\lim_{n \\rightarrow \\infty}  \\left(1-\\frac{1}{n}\\right)^n = \\exp(-1) \\approx 0.3679,\n",
    "$$\n",
    "\n",
    "所以当 $\\beta \\rightarrow 1$时，$\\beta^{1/(1-\\beta)}=\\exp(-1)$，如 $0.95^{20} \\approx \\exp(-1)$。如果把 $\\exp(-1)$ 当作一个比较小的数，我们可以在近似中忽略所有含 $\\beta^{1/(1-\\beta)}$ 和比 $\\beta^{1/(1-\\beta)}$ 更高阶的系数的项。例如，当 $\\beta=0.95$ 时，\n",
    "\n",
    "$$\n",
    "y_t \\approx 0.05 \\sum_{i=0}^{19} 0.95^i x_{t-i}.\n",
    "$$\n",
    "\n",
    "因此，在实际中，我们常常将 $y_t$ 看作是对最近 $1/(1-\\beta)$ 个时间步的 $x_t$ 值的加权平均。例如，当 $\\gamma = 0.95$ 时，$y_t$ 可以被看作对最近20个时间步的 $x_t$ 值的加权平均；当 $\\beta = 0.9$ 时，$y_t$ 可以看作是对最近10个时间步的 $x_t$ 值的加权平均。而且，离当前时间步 $t$ 越近的 $x_t$ 值获得的权重越大（越接近1）。\n",
    "\n",
    "\n",
    "### 由指数加权移动平均理解动量法\n",
    "\n",
    "现在，我们对动量法的速度变量做变形：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{m}_t \\leftarrow \\beta \\boldsymbol{m}_{t-1} + (1 - \\beta) \\left(\\frac{\\eta_t}{1 - \\beta} \\boldsymbol{g}_t\\right). \n",
    "$$\n",
    "\n",
    "Another version:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{m}_t \\leftarrow \\beta \\boldsymbol{m}_{t-1} + (1 - \\beta) \\boldsymbol{g}_t. \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\boldsymbol{x}_t &\\leftarrow \\boldsymbol{x}_{t-1} - \\alpha_t \\boldsymbol{m}_t,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\frac{\\eta_t}{1-\\beta} \n",
    "$$\n",
    "\n",
    "\n",
    "由指数加权移动平均的形式可得，速度变量 $\\boldsymbol{v}_t$ 实际上对序列 $\\{\\eta_{t-i}\\boldsymbol{g}_{t-i} /(1-\\beta):i=0,\\ldots,1/(1-\\beta)-1\\}$ 做了指数加权移动平均。换句话说，相比于小批量随机梯度下降，动量法在每个时间步的自变量更新量近似于将前者对应的最近 $1/(1-\\beta)$ 个时间步的更新量做了指数加权移动平均后再除以 $1-\\beta$。所以，在动量法中，自变量在各个方向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个方向上是否一致。在本节之前示例的优化问题中，所有梯度在水平方向上为正（向右），而在竖直方向上时正（向上）时负（向下）。这样，我们就可以使用较大的学习率，从而使自变量向最优解更快移动。\n",
    "\n",
    "\n",
    "## Implement\n",
    "\n",
    "相对于小批量随机梯度下降，动量法需要对每一个自变量维护一个同它一样形状的速度变量，且超参数里多了动量超参数。实现中，我们将速度变量用更广义的状态变量`states`表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "graffitiCellId": "id_6121f8l",
    "id": "EE50A16E68F142FB8E11D9042EC6E451",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_ch7():  \n",
    "    data = np.genfromtxt('/home/xixixi/桌面/Dive-into-DL-PyTorch/data/airfoil_self_noise.dat', delimiter='\\t')\n",
    "    data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\n",
    "        torch.tensor(data[:1500, -1], dtype=torch.float32)\n",
    "\n",
    "features, labels = get_data_ch7()\n",
    "\n",
    "def init_momentum_states():\n",
    "    v_w = torch.zeros((features.shape[1], 1), dtype=torch.float32)\n",
    "    v_b = torch.zeros(1, dtype=torch.float32)\n",
    "    return (v_w, v_b)\n",
    "\n",
    "def sgd_momentum(params, states, hyperparams):#训练的参数、momentum、超参数beta\n",
    "    for p, v in zip(params, states):\n",
    "        v.data = hyperparams['momentum'] * v.data + hyperparams['lr'] * p.grad.data\n",
    "        p.data -= v.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_2nqiri4",
    "id": "CDAA083E7EF942ED80B11F7B0C3EA69D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "我们先将动量超参数`momentum`设0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "graffitiCellId": "id_oprgtg8",
    "id": "8B8F9D4CBF2F48B88D446C367E97D587",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.243297, 0.057950 sec per epoch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/8B8F9D4CBF2F48B88D446C367E97D587/q5qod8io3b.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.train_ch7(sgd_momentum, init_momentum_states(),\n",
    "              {'lr': 0.02, 'momentum': 0.5}, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_zr2hdkm",
    "id": "BB1884518C734FF59983C832384FCD87",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "将动量超参数`momentum`增大到0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "graffitiCellId": "id_b65o03u",
    "id": "2683F607420E443C951B824B2D8BED83",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.260418, 0.059441 sec per epoch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/2683F607420E443C951B824B2D8BED83/q5qodf7e0m.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.train_ch7(sgd_momentum, init_momentum_states(),\n",
    "              {'lr': 0.02, 'momentum': 0.9}, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_22elqcw",
    "id": "F27F99CE4F6C4BDB842530363BA24572",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "可见目标函数值在后期迭代过程中的变化不够平滑。直觉上，10倍小批量梯度比2倍小批量梯度大了5倍，我们可以试着将学习率减小到原来的1/5。此时目标函数值在下降了一段时间后变化更加平滑。\n",
    "\n",
    "真实的$running rate =  \\frac{\\alpha}{1-\\beta} 即 \\frac{lr}{1-momentum}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "graffitiCellId": "id_9xuazwj",
    "id": "771286646D7A40EEA3D1BDF283C4726A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.243650, 0.063532 sec per epoch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/771286646D7A40EEA3D1BDF283C4726A/q5qodjrkb3.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.train_ch7(sgd_momentum, init_momentum_states(),\n",
    "              {'lr': 0.004, 'momentum': 0.9}, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_d03ehs6",
    "id": "BA4A32D84B7C4B8EA18627A0B3DD50FB",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Pytorch Class\n",
    "\n",
    "在Pytorch中，```torch.optim.SGD```已实现了Momentum。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "graffitiCellId": "id_0ani6vk",
    "id": "061E57E1FC1240988C134FC43E749BEE",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.242484, 0.055843 sec per epoch\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"180.65625pt\" version=\"1.1\" viewBox=\"0 0 246.28125 180.65625\" width=\"246.28125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <defs>\n",
       "  <style type=\"text/css\">\n",
       "*{stroke-linecap:butt;stroke-linejoin:round;}\n",
       "  </style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 180.65625 \n",
       "L 246.28125 180.65625 \n",
       "L 246.28125 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 43.78125 143.1 \n",
       "L 239.08125 143.1 \n",
       "L 239.08125 7.2 \n",
       "L 43.78125 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"m8658005b1f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"52.658523\" xlink:href=\"#m8658005b1f\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0.0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "       <path d=\"M 10.6875 12.40625 \n",
       "L 21 12.40625 \n",
       "L 21 0 \n",
       "L 10.6875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-46\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(44.70696 157.698438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"97.044886\" xlink:href=\"#m8658005b1f\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 0.5 -->\n",
       "      <defs>\n",
       "       <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(89.093324 157.698438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"141.43125\" xlink:href=\"#m8658005b1f\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 1.0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(133.479688 157.698438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"185.817614\" xlink:href=\"#m8658005b1f\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 1.5 -->\n",
       "      <g transform=\"translate(177.866051 157.698438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"230.203977\" xlink:href=\"#m8658005b1f\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 2.0 -->\n",
       "      <defs>\n",
       "       <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(222.252415 157.698438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- epoch -->\n",
       "     <defs>\n",
       "      <path d=\"M 56.203125 29.59375 \n",
       "L 56.203125 25.203125 \n",
       "L 14.890625 25.203125 \n",
       "Q 15.484375 15.921875 20.484375 11.0625 \n",
       "Q 25.484375 6.203125 34.421875 6.203125 \n",
       "Q 39.59375 6.203125 44.453125 7.46875 \n",
       "Q 49.3125 8.734375 54.109375 11.28125 \n",
       "L 54.109375 2.78125 \n",
       "Q 49.265625 0.734375 44.1875 -0.34375 \n",
       "Q 39.109375 -1.421875 33.890625 -1.421875 \n",
       "Q 20.796875 -1.421875 13.15625 6.1875 \n",
       "Q 5.515625 13.8125 5.515625 26.8125 \n",
       "Q 5.515625 40.234375 12.765625 48.109375 \n",
       "Q 20.015625 56 32.328125 56 \n",
       "Q 43.359375 56 49.78125 48.890625 \n",
       "Q 56.203125 41.796875 56.203125 29.59375 \n",
       "z\n",
       "M 47.21875 32.234375 \n",
       "Q 47.125 39.59375 43.09375 43.984375 \n",
       "Q 39.0625 48.390625 32.421875 48.390625 \n",
       "Q 24.90625 48.390625 20.390625 44.140625 \n",
       "Q 15.875 39.890625 15.1875 32.171875 \n",
       "z\n",
       "\" id=\"DejaVuSans-101\"/>\n",
       "      <path d=\"M 18.109375 8.203125 \n",
       "L 18.109375 -20.796875 \n",
       "L 9.078125 -20.796875 \n",
       "L 9.078125 54.6875 \n",
       "L 18.109375 54.6875 \n",
       "L 18.109375 46.390625 \n",
       "Q 20.953125 51.265625 25.265625 53.625 \n",
       "Q 29.59375 56 35.59375 56 \n",
       "Q 45.5625 56 51.78125 48.09375 \n",
       "Q 58.015625 40.1875 58.015625 27.296875 \n",
       "Q 58.015625 14.40625 51.78125 6.484375 \n",
       "Q 45.5625 -1.421875 35.59375 -1.421875 \n",
       "Q 29.59375 -1.421875 25.265625 0.953125 \n",
       "Q 20.953125 3.328125 18.109375 8.203125 \n",
       "z\n",
       "M 48.6875 27.296875 \n",
       "Q 48.6875 37.203125 44.609375 42.84375 \n",
       "Q 40.53125 48.484375 33.40625 48.484375 \n",
       "Q 26.265625 48.484375 22.1875 42.84375 \n",
       "Q 18.109375 37.203125 18.109375 27.296875 \n",
       "Q 18.109375 17.390625 22.1875 11.75 \n",
       "Q 26.265625 6.109375 33.40625 6.109375 \n",
       "Q 40.53125 6.109375 44.609375 11.75 \n",
       "Q 48.6875 17.390625 48.6875 27.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-112\"/>\n",
       "      <path d=\"M 30.609375 48.390625 \n",
       "Q 23.390625 48.390625 19.1875 42.75 \n",
       "Q 14.984375 37.109375 14.984375 27.296875 \n",
       "Q 14.984375 17.484375 19.15625 11.84375 \n",
       "Q 23.34375 6.203125 30.609375 6.203125 \n",
       "Q 37.796875 6.203125 41.984375 11.859375 \n",
       "Q 46.1875 17.53125 46.1875 27.296875 \n",
       "Q 46.1875 37.015625 41.984375 42.703125 \n",
       "Q 37.796875 48.390625 30.609375 48.390625 \n",
       "z\n",
       "M 30.609375 56 \n",
       "Q 42.328125 56 49.015625 48.375 \n",
       "Q 55.71875 40.765625 55.71875 27.296875 \n",
       "Q 55.71875 13.875 49.015625 6.21875 \n",
       "Q 42.328125 -1.421875 30.609375 -1.421875 \n",
       "Q 18.84375 -1.421875 12.171875 6.21875 \n",
       "Q 5.515625 13.875 5.515625 27.296875 \n",
       "Q 5.515625 40.765625 12.171875 48.375 \n",
       "Q 18.84375 56 30.609375 56 \n",
       "z\n",
       "\" id=\"DejaVuSans-111\"/>\n",
       "      <path d=\"M 48.78125 52.59375 \n",
       "L 48.78125 44.1875 \n",
       "Q 44.96875 46.296875 41.140625 47.34375 \n",
       "Q 37.3125 48.390625 33.40625 48.390625 \n",
       "Q 24.65625 48.390625 19.8125 42.84375 \n",
       "Q 14.984375 37.3125 14.984375 27.296875 \n",
       "Q 14.984375 17.28125 19.8125 11.734375 \n",
       "Q 24.65625 6.203125 33.40625 6.203125 \n",
       "Q 37.3125 6.203125 41.140625 7.25 \n",
       "Q 44.96875 8.296875 48.78125 10.40625 \n",
       "L 48.78125 2.09375 \n",
       "Q 45.015625 0.34375 40.984375 -0.53125 \n",
       "Q 36.96875 -1.421875 32.421875 -1.421875 \n",
       "Q 20.0625 -1.421875 12.78125 6.34375 \n",
       "Q 5.515625 14.109375 5.515625 27.296875 \n",
       "Q 5.515625 40.671875 12.859375 48.328125 \n",
       "Q 20.21875 56 33.015625 56 \n",
       "Q 37.15625 56 41.109375 55.140625 \n",
       "Q 45.0625 54.296875 48.78125 52.59375 \n",
       "z\n",
       "\" id=\"DejaVuSans-99\"/>\n",
       "      <path d=\"M 54.890625 33.015625 \n",
       "L 54.890625 0 \n",
       "L 45.90625 0 \n",
       "L 45.90625 32.71875 \n",
       "Q 45.90625 40.484375 42.875 44.328125 \n",
       "Q 39.84375 48.1875 33.796875 48.1875 \n",
       "Q 26.515625 48.1875 22.3125 43.546875 \n",
       "Q 18.109375 38.921875 18.109375 30.90625 \n",
       "L 18.109375 0 \n",
       "L 9.078125 0 \n",
       "L 9.078125 75.984375 \n",
       "L 18.109375 75.984375 \n",
       "L 18.109375 46.1875 \n",
       "Q 21.34375 51.125 25.703125 53.5625 \n",
       "Q 30.078125 56 35.796875 56 \n",
       "Q 45.21875 56 50.046875 50.171875 \n",
       "Q 54.890625 44.34375 54.890625 33.015625 \n",
       "z\n",
       "\" id=\"DejaVuSans-104\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(126.203125 171.376563)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-101\"/>\n",
       "      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n",
       "      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n",
       "      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"ma42e4302f4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma42e4302f4\" y=\"103.197137\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0.4 -->\n",
       "      <defs>\n",
       "       <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 106.996356)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma42e4302f4\" y=\"60.375454\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.6 -->\n",
       "      <defs>\n",
       "       <path d=\"M 33.015625 40.375 \n",
       "Q 26.375 40.375 22.484375 35.828125 \n",
       "Q 18.609375 31.296875 18.609375 23.390625 \n",
       "Q 18.609375 15.53125 22.484375 10.953125 \n",
       "Q 26.375 6.390625 33.015625 6.390625 \n",
       "Q 39.65625 6.390625 43.53125 10.953125 \n",
       "Q 47.40625 15.53125 47.40625 23.390625 \n",
       "Q 47.40625 31.296875 43.53125 35.828125 \n",
       "Q 39.65625 40.375 33.015625 40.375 \n",
       "z\n",
       "M 52.59375 71.296875 \n",
       "L 52.59375 62.3125 \n",
       "Q 48.875 64.0625 45.09375 64.984375 \n",
       "Q 41.3125 65.921875 37.59375 65.921875 \n",
       "Q 27.828125 65.921875 22.671875 59.328125 \n",
       "Q 17.53125 52.734375 16.796875 39.40625 \n",
       "Q 19.671875 43.65625 24.015625 45.921875 \n",
       "Q 28.375 48.1875 33.59375 48.1875 \n",
       "Q 44.578125 48.1875 50.953125 41.515625 \n",
       "Q 57.328125 34.859375 57.328125 23.390625 \n",
       "Q 57.328125 12.15625 50.6875 5.359375 \n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \n",
       "Q 6.984375 17.96875 6.984375 36.375 \n",
       "Q 6.984375 53.65625 15.1875 63.9375 \n",
       "Q 23.390625 74.21875 37.203125 74.21875 \n",
       "Q 40.921875 74.21875 44.703125 73.484375 \n",
       "Q 48.484375 72.75 52.59375 71.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-54\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 64.174673)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#ma42e4302f4\" y=\"17.553771\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.8 -->\n",
       "      <defs>\n",
       "       <path d=\"M 31.78125 34.625 \n",
       "Q 24.75 34.625 20.71875 30.859375 \n",
       "Q 16.703125 27.09375 16.703125 20.515625 \n",
       "Q 16.703125 13.921875 20.71875 10.15625 \n",
       "Q 24.75 6.390625 31.78125 6.390625 \n",
       "Q 38.8125 6.390625 42.859375 10.171875 \n",
       "Q 46.921875 13.96875 46.921875 20.515625 \n",
       "Q 46.921875 27.09375 42.890625 30.859375 \n",
       "Q 38.875 34.625 31.78125 34.625 \n",
       "z\n",
       "M 21.921875 38.8125 \n",
       "Q 15.578125 40.375 12.03125 44.71875 \n",
       "Q 8.5 49.078125 8.5 55.328125 \n",
       "Q 8.5 64.0625 14.71875 69.140625 \n",
       "Q 20.953125 74.21875 31.78125 74.21875 \n",
       "Q 42.671875 74.21875 48.875 69.140625 \n",
       "Q 55.078125 64.0625 55.078125 55.328125 \n",
       "Q 55.078125 49.078125 51.53125 44.71875 \n",
       "Q 48 40.375 41.703125 38.8125 \n",
       "Q 48.828125 37.15625 52.796875 32.3125 \n",
       "Q 56.78125 27.484375 56.78125 20.515625 \n",
       "Q 56.78125 9.90625 50.3125 4.234375 \n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \n",
       "Q 19.734375 -1.421875 13.25 4.234375 \n",
       "Q 6.78125 9.90625 6.78125 20.515625 \n",
       "Q 6.78125 27.484375 10.78125 32.3125 \n",
       "Q 14.796875 37.15625 21.921875 38.8125 \n",
       "z\n",
       "M 18.3125 54.390625 \n",
       "Q 18.3125 48.734375 21.84375 45.5625 \n",
       "Q 25.390625 42.390625 31.78125 42.390625 \n",
       "Q 38.140625 42.390625 41.71875 45.5625 \n",
       "Q 45.3125 48.734375 45.3125 54.390625 \n",
       "Q 45.3125 60.0625 41.71875 63.234375 \n",
       "Q 38.140625 66.40625 31.78125 66.40625 \n",
       "Q 25.390625 66.40625 21.84375 63.234375 \n",
       "Q 18.3125 60.0625 18.3125 54.390625 \n",
       "z\n",
       "\" id=\"DejaVuSans-56\"/>\n",
       "      </defs>\n",
       "      <g transform=\"translate(20.878125 21.35299)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_10\">\n",
       "     <!-- loss -->\n",
       "     <defs>\n",
       "      <path d=\"M 9.421875 75.984375 \n",
       "L 18.40625 75.984375 \n",
       "L 18.40625 0 \n",
       "L 9.421875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-108\"/>\n",
       "      <path d=\"M 44.28125 53.078125 \n",
       "L 44.28125 44.578125 \n",
       "Q 40.484375 46.53125 36.375 47.5 \n",
       "Q 32.28125 48.484375 27.875 48.484375 \n",
       "Q 21.1875 48.484375 17.84375 46.4375 \n",
       "Q 14.5 44.390625 14.5 40.28125 \n",
       "Q 14.5 37.15625 16.890625 35.375 \n",
       "Q 19.28125 33.59375 26.515625 31.984375 \n",
       "L 29.59375 31.296875 \n",
       "Q 39.15625 29.25 43.1875 25.515625 \n",
       "Q 47.21875 21.78125 47.21875 15.09375 \n",
       "Q 47.21875 7.46875 41.1875 3.015625 \n",
       "Q 35.15625 -1.421875 24.609375 -1.421875 \n",
       "Q 20.21875 -1.421875 15.453125 -0.5625 \n",
       "Q 10.6875 0.296875 5.421875 2 \n",
       "L 5.421875 11.28125 \n",
       "Q 10.40625 8.6875 15.234375 7.390625 \n",
       "Q 20.0625 6.109375 24.8125 6.109375 \n",
       "Q 31.15625 6.109375 34.5625 8.28125 \n",
       "Q 37.984375 10.453125 37.984375 14.40625 \n",
       "Q 37.984375 18.0625 35.515625 20.015625 \n",
       "Q 33.0625 21.96875 24.703125 23.78125 \n",
       "L 21.578125 24.515625 \n",
       "Q 13.234375 26.265625 9.515625 29.90625 \n",
       "Q 5.8125 33.546875 5.8125 39.890625 \n",
       "Q 5.8125 47.609375 11.28125 51.796875 \n",
       "Q 16.75 56 26.8125 56 \n",
       "Q 31.78125 56 36.171875 55.265625 \n",
       "Q 40.578125 54.546875 44.28125 53.078125 \n",
       "z\n",
       "\" id=\"DejaVuSans-115\"/>\n",
       "     </defs>\n",
       "     <g transform=\"translate(14.798437 84.807812)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <use xlink:href=\"#DejaVuSans-108\"/>\n",
       "      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n",
       "      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_9\">\n",
       "    <path clip-path=\"url(#p8f100b7436)\" d=\"M 52.658523 13.377273 \n",
       "L 58.576705 31.289737 \n",
       "L 64.494886 54.805984 \n",
       "L 70.413068 71.880109 \n",
       "L 76.33125 90.38073 \n",
       "L 82.249432 101.326069 \n",
       "L 88.167614 110.03365 \n",
       "L 94.085795 116.560624 \n",
       "L 100.003977 121.443917 \n",
       "L 105.922159 124.501587 \n",
       "L 111.840341 127.127837 \n",
       "L 117.758523 129.278529 \n",
       "L 123.676705 131.648035 \n",
       "L 129.594886 133.167041 \n",
       "L 135.513068 134.000575 \n",
       "L 141.43125 134.699114 \n",
       "L 147.349432 134.920947 \n",
       "L 153.267614 135.43264 \n",
       "L 159.185795 135.795176 \n",
       "L 165.103977 136.198096 \n",
       "L 171.022159 136.241247 \n",
       "L 176.940341 136.36163 \n",
       "L 182.858523 136.551766 \n",
       "L 188.776705 136.526028 \n",
       "L 194.694886 136.699755 \n",
       "L 200.613068 136.731714 \n",
       "L 206.53125 136.75213 \n",
       "L 212.449432 136.904558 \n",
       "L 218.367614 136.91582 \n",
       "L 224.285795 136.775596 \n",
       "L 230.203977 136.922727 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 43.78125 143.1 \n",
       "L 43.78125 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 239.08125 143.1 \n",
       "L 239.08125 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 43.78125 143.1 \n",
       "L 239.08125 143.1 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 43.78125 7.2 \n",
       "L 239.08125 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p8f100b7436\">\n",
       "   <rect height=\"135.9\" width=\"195.3\" x=\"43.78125\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.train_pytorch_ch7(torch.optim.SGD, {'lr': 0.004, 'momentum': 0.8},\n",
    "                    features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "graffitiCellId": "id_6t7yxla",
    "id": "9760485882F04988819911A12BEF3FE4",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 11.7 AdaGrad\n",
    "\n",
    "在之前介绍过的优化算法中，目标函数自变量的每一个元素在相同时间步都使用同一个学习率来自我迭代。举个例子，假设目标函数为$f$，自变量为一个二维向量$[x_1, x_2]^\\top$，该向量中每一个元素在迭代时都使用相同的学习率。例如，在学习率为$\\eta$的梯度下降中，元素$x_1$和$x_2$都使用相同的学习率$\\eta$来自我迭代：\n",
    "\n",
    "\n",
    "$$\n",
    "x_1 \\leftarrow x_1 - \\eta \\frac{\\partial{f}}{\\partial{x_1}}, \\quad\n",
    "x_2 \\leftarrow x_2 - \\eta \\frac{\\partial{f}}{\\partial{x_2}}.\n",
    "$$\n",
    "\n",
    "\n",
    "在[“动量法”](./momentum.ipynb)一节里我们看到当$x_1$和$x_2$的梯度值有较大差别时，需要选择足够小的学习率使得自变量在梯度值较大的维度上不发散。但这样会导致自变量在梯度值较小的维度上迭代过慢。   \n",
    "**动量法依赖指数加权移动平均使得自变量的更新方向更加一致，从而降低发散的可能。   \n",
    "AdaGrad算法：它根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题 。**\n",
    "\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "AdaGrad算法会使用一个小批量随机梯度$\\boldsymbol{g}_t$按元素平方的累加变量$\\boldsymbol{s}_t$。在时间步0，AdaGrad将$\\boldsymbol{s}_0$中每个元素初始化为0。在时间步$t$，首先将小批量随机梯度$\\boldsymbol{g}_t$按元素平方后累加到变量$\\boldsymbol{s}_t$：\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{s}_t \\leftarrow \\boldsymbol{s}_{t-1} + \\boldsymbol{g}_t \\odot \\boldsymbol{g}_t,\n",
    "$$\n",
    "\n",
    "\n",
    "其中$\\odot$是按元素相乘。接着，我们将目标函数自变量中每个元素的学习率通过按元素运算重新调整一下：\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_t \\leftarrow \\boldsymbol{x}_{t-1} - \\frac{\\eta}{\\sqrt{\\boldsymbol{s}_t + \\epsilon}} \\odot \\boldsymbol{g}_t,\n",
    "$$\n",
    "\n",
    "\n",
    "其中$\\eta$是学习率，$\\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。这里开方、除法和乘法的运算都是按元素运算的。这些按元素运算使得目标函数自变量中每个元素都分别拥有自己的学习率。梯度幅值较大方向，对应的$s_t$较大，相对应的学习率就会变小。\n",
    "\n",
    "## Feature\n",
    "\n",
    "需要强调的是，小批量随机梯度按元素平方的累加变量$\\boldsymbol{s}_t$出现在学习率的分母项中。因此，如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。然而，由于$\\boldsymbol{s}_t$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。\n",
    "\n",
    "下面我们仍然以目标函数$f(\\boldsymbol{x})=0.1x_1^2+2x_2^2$为例观察AdaGrad算法对自变量的迭代轨迹。我们实现AdaGrad算法并使用和上一节实验中相同的学习率0.4。可以看到，自变量的迭代轨迹较平滑。但由于$\\boldsymbol{s}_t$的累加效果使学习率不断衰减，自变量在迭代后期的移动幅度较小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "graffitiCellId": "id_8yhntry",
    "id": "65D88109B129448EB6DAC9C0A04110BF",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, x1 -2.382563, x2 -0.158591\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/65D88109B129448EB6DAC9C0A04110BF/q5qoefd6ox.svg\">"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/home/kesci/input\") \n",
    "import d2lzh1981 as d2l\n",
    "\n",
    "def adagrad_2d(x1, x2, s1, s2):\n",
    "    g1, g2, eps = 0.2 * x1, 4 * x2, 1e-6  # 前两项为自变量梯度\n",
    "    s1 += g1 ** 2\n",
    "    s2 += g2 ** 2\n",
    "    x1 -= eta / math.sqrt(s1 + eps) * g1\n",
    "    x2 -= eta / math.sqrt(s2 + eps) * g2\n",
    "    return x1, x2, s1, s2\n",
    "\n",
    "def f_2d(x1, x2):\n",
    "    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "eta = 0.4\n",
    "d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "graffitiCellId": "id_v2qefl3",
    "id": "1886AE635E6E4F11808F463063D9CFF2",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "下面将学习率增大到2。可以看到自变量更为迅速地逼近了最优解。   \n",
    "大学习率容易导致发散。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "graffitiCellId": "id_0nyfecw",
    "id": "90B791EDF32649498EB29AFD2D77302A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, x1 -0.002295, x2 -0.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/90B791EDF32649498EB29AFD2D77302A/q5qoekdeom.svg\">"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eta = 2\n",
    "d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_3v4zc27",
    "id": "3ACFF070C6424B32A01EAA8303927C81",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Implement\n",
    "\n",
    "同动量法一样，AdaGrad算法需要对每个自变量维护同它一样形状的状态变量。我们根据AdaGrad算法中的公式实现该算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "graffitiCellId": "id_vpq6w6w",
    "id": "B7D3D3F5289548009A73E0576F10D07E",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_ch7():  \n",
    "    data = np.genfromtxt('/home/kesci/input/airfoil4755/airfoil_self_noise.dat', delimiter='\\t')\n",
    "    data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\n",
    "        torch.tensor(data[:1500, -1], dtype=torch.float32)\n",
    "        \n",
    "features, labels = get_data_ch7()\n",
    "\n",
    "def init_adagrad_states():\n",
    "    s_w = torch.zeros((features.shape[1], 1), dtype=torch.float32)\n",
    "    s_b = torch.zeros(1, dtype=torch.float32)\n",
    "    return (s_w, s_b)\n",
    "\n",
    "def adagrad(params, states, hyperparams):\n",
    "    eps = 1e-6\n",
    "    for p, s in zip(params, states):\n",
    "        s.data += (p.grad.data**2)\n",
    "        p.data -= hyperparams['lr'] * p.grad.data / torch.sqrt(s + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_htom2jh",
    "id": "6050B51756C7426392ABF086AC9C1E00",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "使用更大的学习率来训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "graffitiCellId": "id_khjyikm",
    "id": "FB3ACF978EAE4A158BFCC322169D396C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.242258, 0.061548 sec per epoch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/FB3ACF978EAE4A158BFCC322169D396C/q5qofl6l7n.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.train_ch7(adagrad, init_adagrad_states(), {'lr': 0.1}, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_dtj8dbu",
    "id": "E77565E98BC04932B7064076F1522043",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Pytorch Class\n",
    "\n",
    "通过名称为“adagrad”的`Trainer`实例，我们便可使用Pytorch提供的AdaGrad算法来训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "graffitiCellId": "id_amhbtt1",
    "id": "9ADC04FA976240DE8656E060A4B98F49",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.243800, 0.060953 sec per epoch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/9ADC04FA976240DE8656E060A4B98F49/q5qofropkx.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.train_pytorch_ch7(torch.optim.Adagrad, {'lr': 0.1}, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_qo3gwz9",
    "id": "8AFBDBA2897F424984F2EC7EE90BD2C9",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 11.8 RMSProp\n",
    "\n",
    "[“AdaGrad算法”](adagrad.ipynb)一节中提到，因为调整学习率时分母上的变量$\\boldsymbol{s}_t$一直在累加按元素平方的小批量随机梯度，所以目标函数自变量每个元素的学习率在迭代过程中一直在降低（或不变）。因此，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSProp算法对AdaGrad算法做了修改。该算法源自Coursera上的一门课程，即“机器学习的神经网络”。\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "[“动量法”](momentum.ipynb)一节里介绍过指数加权移动平均。不同于AdaGrad算法里状态变量$\\boldsymbol{s}_t$是截至时间步$t$所有小批量随机梯度$\\boldsymbol{g}_t$按元素平方和，RMSProp算法将这些梯度按元素平方做指数加权移动平均。具体来说，给定超参数$0 \\leq \\gamma < 1$，RMSProp算法在时间步$t > 0$ 计算\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v}_t \\leftarrow \\beta \\boldsymbol{v}_{t-1} + (1 - \\beta) \\boldsymbol{g}_t \\odot \\boldsymbol{g}_t. \n",
    "$$\n",
    "\n",
    "\n",
    "和AdaGrad算法一样，RMSProp算法将目标函数自变量中每个元素的学习率通过按元素运算重新调整，然后更新自变量\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_t \\leftarrow \\boldsymbol{x}_{t-1} - \\frac{\\alpha}{\\sqrt{\\boldsymbol{v}_t + \\epsilon}} \\odot \\boldsymbol{g}_t, \n",
    "$$\n",
    "\n",
    "\n",
    "其中$\\eta$是学习率，$\\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-6}$。因为RMSProp算法的状态变量$\\boldsymbol{s}_t$是对平方项$\\boldsymbol{g}_t \\odot \\boldsymbol{g}_t$的指数加权移动平均，所以可以看作是最近$1/(1-\\beta)$个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。\n",
    "\n",
    "照例，让我们先观察RMSProp算法对目标函数$f(\\boldsymbol{x})=0.1x_1^2+2x_2^2$中自变量的迭代轨迹。在[“AdaGrad算法”](adagrad.ipynb)一节使用的学习率为0.4的AdaGrad算法，自变量在迭代后期的移动幅度较小。但在同样的学习率下，RMSProp算法可以更快逼近最优解。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "graffitiCellId": "id_zto35ht",
    "id": "488520FCB5BC4DFF811770D00333B399",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, x1 -0.010599, x2 0.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/488520FCB5BC4DFF811770D00333B399/q5qog9m8u0.svg\">"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/home/xixixi/桌面/Dive-into-DL-PyTorch/code\")\n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "def rmsprop_2d(x1, x2, s1, s2):\n",
    "    g1, g2, eps = 0.2 * x1, 4 * x2, 1e-6\n",
    "    s1 = beta * s1 + (1 - beta) * g1 ** 2\n",
    "    s2 = beta * s2 + (1 - beta) * g2 ** 2\n",
    "    x1 -= alpha / math.sqrt(s1 + eps) * g1\n",
    "    x2 -= alpha / math.sqrt(s2 + eps) * g2\n",
    "    return x1, x2, s1, s2\n",
    "\n",
    "def f_2d(x1, x2):\n",
    "    return 0.1 * x1 ** 2 + 2 * x2 ** 2\n",
    "\n",
    "alpha, beta = 0.4, 0.9\n",
    "d2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_xu3o2of",
    "id": "0AF3943AD95549A886EB0FA31A8DB0C5",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Implement\n",
    "\n",
    "接下来按照RMSProp算法中的公式实现该算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "graffitiCellId": "id_nxkci37",
    "id": "2F6F0D4E82CF4A2AA4501508A6CC7B36",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_ch7():  \n",
    "    data = np.genfromtxt('/home/kesci/input/airfoil4755/airfoil_self_noise.dat', delimiter='\\t')\n",
    "    data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\n",
    "        torch.tensor(data[:1500, -1], dtype=torch.float32)\n",
    "        \n",
    "features, labels = get_data_ch7()\n",
    "\n",
    "def init_rmsprop_states():\n",
    "    s_w = torch.zeros((features.shape[1], 1), dtype=torch.float32)\n",
    "    s_b = torch.zeros(1, dtype=torch.float32)\n",
    "    return (s_w, s_b)\n",
    "\n",
    "def rmsprop(params, states, hyperparams):\n",
    "    gamma, eps = hyperparams['beta'], 1e-6\n",
    "    for p, s in zip(params, states):\n",
    "        s.data = gamma * s.data + (1 - gamma) * (p.grad.data)**2\n",
    "        p.data -= hyperparams['lr'] * p.grad.data / torch.sqrt(s + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_qfltkd7",
    "id": "9A045BE025B94F30B4DB9643CE89899F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "我们将初始学习率设为0.01，并将超参数$\\gamma$设为0.9。此时，变量$\\boldsymbol{s}_t$可看作是最近$1/(1-0.9) = 10$个时间步的平方项$\\boldsymbol{g}_t \\odot \\boldsymbol{g}_t$的加权平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "graffitiCellId": "id_kexvzh2",
    "id": "5C9361F719B844808D67652F774041F3",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.243334, 0.063004 sec per epoch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/5C9361F719B844808D67652F774041F3/q5qogvxs90.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.train_ch7(rmsprop, init_rmsprop_states(), {'lr': 0.01, 'beta': 0.9},\n",
    "              features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_qizxgr0",
    "id": "6DF5AA8844D942788DDCC755BB032D0B",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Pytorch Class\n",
    "\n",
    "通过名称为“rmsprop”的`Trainer`实例，我们便可使用Gluon提供的RMSProp算法来训练模型。注意，超参数$\\gamma$通过`gamma1`指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "graffitiCellId": "id_jdtorb2",
    "id": "B18281B434DC4DAD833ADF5A911D81C2",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.244934, 0.062977 sec per epoch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/B18281B434DC4DAD833ADF5A911D81C2/q5qoh04h4o.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.train_pytorch_ch7(torch.optim.RMSprop, {'lr': 0.01, 'alpha': 0.9},\n",
    "                    features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_c7rew13",
    "id": "F7B2EB3E76834E4CA1FE2BB42A009B7B",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 11.9 AdaDelta\n",
    "\n",
    "除了RMSProp算法以外，另一个常用优化算法AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有用解的问题做了改进 。有意思的是，**AdaDelta算法没有学习率这一超参数。**\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "AdaDelta算法也像RMSProp算法一样，使用了小批量随机梯度$\\boldsymbol{g}_t$按元素平方的指数加权移动平均变量$\\boldsymbol{s}_t$。在时间步0，它的所有元素被初始化为0。给定超参数$0 \\leq \\rho 0$，同RMSProp算法一样计算\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{s}_t \\leftarrow \\rho \\boldsymbol{s}_{t-1} + (1 - \\rho) \\boldsymbol{g}_t \\odot \\boldsymbol{g}_t. \n",
    "$$\n",
    "\n",
    "\n",
    "与RMSProp算法不同的是，AdaDelta算法还维护一个额外的状态变量$\\Delta\\boldsymbol{x}_t$，其元素同样在时间步0时被初始化为0。我们使用$\\Delta\\boldsymbol{x}_{t-1}$来计算自变量的变化量：\n",
    "\n",
    "\n",
    "$$\n",
    " \\boldsymbol{g}_t' \\leftarrow \\sqrt{\\frac{\\Delta\\boldsymbol{x}_{t-1} + \\epsilon}{\\boldsymbol{s}_t + \\epsilon}}   \\odot \\boldsymbol{g}_t, \n",
    "$$\n",
    "\n",
    "\n",
    "其中$\\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-5}$。接着更新自变量：\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_t \\leftarrow \\boldsymbol{x}_{t-1} - \\boldsymbol{g}'_t. \n",
    "$$\n",
    "\n",
    "\n",
    "最后，我们使用$\\Delta\\boldsymbol{x}_t$来记录自变量变化量$\\boldsymbol{g}'_t$按元素平方的指数加权移动平均：\n",
    "\n",
    "\n",
    "$$\n",
    "\\Delta\\boldsymbol{x}_t \\leftarrow \\rho \\Delta\\boldsymbol{x}_{t-1} + (1 - \\rho) \\boldsymbol{g}'_t \\odot \\boldsymbol{g}'_t. \n",
    "$$\n",
    "\n",
    "\n",
    "可以看到，如不考虑$\\epsilon$的影响，AdaDelta算法与RMSProp算法的不同之处在于使用$\\sqrt{\\Delta\\boldsymbol{x}_{t-1}}$来替代超参数$\\eta$。\n",
    "\n",
    "\n",
    "## Implement\n",
    "\n",
    "AdaDelta算法需要对每个自变量维护两个状态变量，即$\\boldsymbol{s}_t$和$\\Delta\\boldsymbol{x}_t$。我们按AdaDelta算法中的公式实现该算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "graffitiCellId": "id_xl6t57y",
    "id": "4C7FB2511AA84AFE81018F63142B409A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_adadelta_states():\n",
    "    s_w, s_b = torch.zeros((features.shape[1], 1), dtype=torch.float32), torch.zeros(1, dtype=torch.float32)\n",
    "    delta_w, delta_b = torch.zeros((features.shape[1], 1), dtype=torch.float32), torch.zeros(1, dtype=torch.float32)\n",
    "    return ((s_w, delta_w), (s_b, delta_b))\n",
    "\n",
    "def adadelta(params, states, hyperparams):\n",
    "    rho, eps = hyperparams['rho'], 1e-5\n",
    "    for p, (s, delta) in zip(params, states):\n",
    "        s[:] = rho * s + (1 - rho) * (p.grad.data**2)\n",
    "        g =  p.grad.data * torch.sqrt((delta + eps) / (s + eps))\n",
    "        p.data -= g\n",
    "        delta[:] = rho * delta + (1 - rho) * g * g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "graffitiCellId": "id_r6rxlkq",
    "id": "48D75FB92AAB4D568DD1A3FBA62408EF",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.243485, 0.084914 sec per epoch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/48D75FB92AAB4D568DD1A3FBA62408EF/q5qohc7hny.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.train_ch7(adadelta, init_adadelta_states(), {'rho': 0.9}, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_80oczfx",
    "id": "84853E8123E142C0B167E9D80F20B388",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Pytorch Class\n",
    "\n",
    "通过名称为“adadelta”的`Trainer`实例，我们便可使用pytorch提供的AdaDelta算法。它的超参数可以通过`rho`来指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "graffitiCellId": "id_hcygf4b",
    "id": "8E66D8902B3045AAABC2D59F2CFA73A0",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.267756, 0.061329 sec per epoch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/8E66D8902B3045AAABC2D59F2CFA73A0/q5qohjtwx7.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.train_pytorch_ch7(torch.optim.Adadelta, {'rho': 0.9}, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_jrm85ia",
    "id": "AE21D5602CE74DBBAE6C4BE698B56B97",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 11.10 Adam\n",
    " \n",
    "Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。   \n",
    "优点：无论优化表面形状如何，Adam总是以一个很正的方向指向最优点，而不是局限在当前梯度的方向。   \n",
    "缺点：很强的rescale性质 会导致不收敛。\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "Adam算法使用了动量变量$\\boldsymbol{m}_t$和RMSProp算法中小批量随机梯度按元素平方的指数加权移动平均变量$\\boldsymbol{v}_t$，并在时间步0将它们中每个元素初始化为0。给定超参数$0 \\leq \\beta_1 < 1$（算法作者建议设为0.9），时间步$t$的动量变量$\\boldsymbol{m}_t$即小批量随机梯度$\\boldsymbol{g}_t$的指数加权移动平均：\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{m}_t \\leftarrow \\beta_1 \\boldsymbol{m}_{t-1} + (1 - \\beta_1) \\boldsymbol{g}_t. \n",
    "$$\n",
    "\n",
    "$m_t$相当于梯度一阶矩（期望）的估计\n",
    "\n",
    "和RMSProp算法中一样，给定超参数$0 \\leq \\beta_2 < 1$（算法作者建议设为0.999），\n",
    "将小批量随机梯度按元素平方后的项$\\boldsymbol{g}_t \\odot \\boldsymbol{g}_t$做指数加权移动平均得到$\\boldsymbol{v}_t$：\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{v}_t \\leftarrow \\beta_2 \\boldsymbol{v}_{t-1} + (1 - \\beta_2) \\boldsymbol{g}_t \\odot \\boldsymbol{g}_t. \n",
    "$$\n",
    "\n",
    "$v_t$相当于梯度二阶矩（平方的期望）的估计\n",
    "\n",
    "由于我们将$\\boldsymbol{m}_0$和$\\boldsymbol{s}_0$中的元素都初始化为0，\n",
    "在时间步$t$我们得到$\\boldsymbol{m}_t =  (1-\\beta_1) \\sum_{i=1}^t \\beta_1^{t-i} \\boldsymbol{g}_i$。将过去各时间步小批量随机梯度的权值相加，得到 $(1-\\beta_1) \\sum_{i=1}^t \\beta_1^{t-i} = 1 - \\beta_1^t$。需要注意的是，当$t$较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当$\\beta_1 = 0.9$时，$\\boldsymbol{m}_1 = 0.1\\boldsymbol{g}_1$。为了消除这样的影响，对于任意时间步$t$，我们可以将$\\boldsymbol{m}_t$再除以$1 - \\beta_1^t$，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作偏差修正。在Adam算法中，我们对变量$\\boldsymbol{m}_t$和$\\boldsymbol{v}_t$均作偏差修正：\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{m}}_t \\leftarrow \\frac{\\boldsymbol{m}_t}{1 - \\beta_1^t}, \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{v}}_t \\leftarrow \\frac{\\boldsymbol{v}_t}{1 - \\beta_2^t}. \n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "接下来，Adam算法使用以上偏差修正后的变量$\\hat{\\boldsymbol{m}}_t$和$\\hat{\\boldsymbol{m}}_t$，将模型参数中每个元素的学习率通过按元素运算重新调整：\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{g}_t' \\leftarrow \\frac{\\eta \\hat{\\boldsymbol{m}}_t}{\\sqrt{\\hat{\\boldsymbol{v}}_t} + \\epsilon},\n",
    "$$\n",
    "\n",
    "无论${g}_t$等于多少，${g}_t'$ 都会接近1\n",
    "\n",
    "其中$\\eta$是学习率，$\\epsilon$是为了维持数值稳定性而添加的常数，如$10^{-8}$。和AdaGrad算法、RMSProp算法以及AdaDelta算法一样，目标函数自变量中每个元素都分别拥有自己的学习率。最后，使用$\\boldsymbol{g}_t'$迭代自变量：\n",
    "\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_t \\leftarrow \\boldsymbol{x}_{t-1} - \\boldsymbol{g}_t'. \n",
    "$$\n",
    "\n",
    "\n",
    "## Implement\n",
    "\n",
    "我们按照Adam算法中的公式实现该算法。其中时间步$t$通过`hyperparams`参数传入`adam`函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "graffitiCellId": "id_ocfckgm",
    "id": "EE3231A89C3740649E0D930730DEF0EA",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/home/kesci/input\") \n",
    "import d2lzh1981 as d2l\n",
    "\n",
    "def get_data_ch7():  \n",
    "    data = np.genfromtxt('/home/kesci/input/airfoil4755/airfoil_self_noise.dat', delimiter='\\t')\n",
    "    data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "    return torch.tensor(data[:1500, :-1], dtype=torch.float32), \\\n",
    "        torch.tensor(data[:1500, -1], dtype=torch.float32)\n",
    "        \n",
    "features, labels = get_data_ch7()\n",
    "\n",
    "def init_adam_states():\n",
    "    v_w, v_b = torch.zeros((features.shape[1], 1), dtype=torch.float32), torch.zeros(1, dtype=torch.float32)\n",
    "    s_w, s_b = torch.zeros((features.shape[1], 1), dtype=torch.float32), torch.zeros(1, dtype=torch.float32)\n",
    "    return ((v_w, s_w), (v_b, s_b))\n",
    "\n",
    "def adam(params, states, hyperparams):\n",
    "    beta1, beta2, eps = 0.9, 0.999, 1e-6\n",
    "    for p, (v, s) in zip(params, states):\n",
    "        v[:] = beta1 * v + (1 - beta1) * p.grad.data\n",
    "        s[:] = beta2 * s + (1 - beta2) * p.grad.data**2\n",
    "        v_bias_corr = v / (1 - beta1 ** hyperparams['t'])\n",
    "        s_bias_corr = s / (1 - beta2 ** hyperparams['t'])\n",
    "        p.data -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr) + eps)\n",
    "    hyperparams['t'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "graffitiCellId": "id_t58avk9",
    "id": "46DA5110F99A4BB58180C0D38497C943",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.242722, 0.089254 sec per epoch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/46DA5110F99A4BB58180C0D38497C943/q5qoij5h08.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.train_ch7(adam, init_adam_states(), {'lr': 0.01, 't': 1}, features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "graffitiCellId": "id_0kb25w1",
    "id": "0D75404B5C874A118C6BF38DA7B749A9",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Pytorch Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "graffitiCellId": "id_phw50au",
    "id": "8E491F60A4FB4C2990C60CF86E00BAF1",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.242389, 0.073228 sec per epoch\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/8E491F60A4FB4C2990C60CF86E00BAF1/q5qoio531k.svg\">"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d2l.train_pytorch_ch7(torch.optim.Adam, {'lr': 0.01}, features, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
